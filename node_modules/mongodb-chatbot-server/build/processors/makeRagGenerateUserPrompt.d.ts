import { FindContentFunc } from "../routes";
import { GenerateUserPromptFunc } from "./GenerateUserPromptFunc";
import { QueryPreprocessorFunc } from "./QueryPreprocessorFunc";
import { UserMessage } from "../services";
import { EmbeddedContent } from "mongodb-rag-core";
import { MakeReferenceLinksFunc } from "./MakeReferenceLinksFunc";
export interface MakeRagGenerateUserPromptParams {
    /**
      Transform the user's message before sending it to the `findContent` function.
     */
    queryPreprocessor?: QueryPreprocessorFunc;
    /**
      Find content based on the user's message and preprocessing.
     */
    findContent: FindContentFunc;
    /**
      If not specified, uses {@link makeDefaultReferenceLinks}.
     */
    makeReferenceLinks?: MakeReferenceLinksFunc;
    /**
      Number of tokens from the found context to send to the `makeUserMessage` function.
      All chunks that exceed this threshold are discarded.
      If not specified, uses {@link DEFAULT_MAX_CONTEXT_TOKENS}.
     */
    maxChunkContextTokens?: number;
    /**
      Construct user message which is sent to the LLM and stored in the database.
     */
    makeUserMessage: MakeUserMessageFunc;
}
export interface MakeUserMessageFuncParams {
    content: EmbeddedContent[];
    originalUserMessage: string;
    preprocessedUserMessage?: string;
    queryEmbedding?: number[];
    rejectQuery?: boolean;
}
export type MakeUserMessageFunc = (params: MakeUserMessageFuncParams) => Promise<UserMessage>;
/**
  Construct a {@link GenerateUserPromptFunc} function
  that uses retrieval augmented generation (RAG) to generate the user prompt
  and return references to use in the answer.
  The returned RAG user prompt generator performs the following steps:
  1. Preprocess the user's message using the query preprocessor.
  2. Find content using vector search.
  3. Removes any chunks that would exceed the max context tokens.
  4. Generate the user message using the make user message function.
  5. Return the user message and references.                                                                                                                */
export declare function makeRagGenerateUserPrompt({ queryPreprocessor, findContent, makeReferenceLinks, maxChunkContextTokens, makeUserMessage, }: MakeRagGenerateUserPromptParams): GenerateUserPromptFunc;
/**
      This function returns the chunks that can fit in the maxTokens.
      It limits the number of tokens that are sent to the LLM.
      */
export declare function includeChunksForMaxTokensPossible({ maxTokens, content, }: {
    maxTokens: number;
    content: EmbeddedContent[];
}): EmbeddedContent[];
//# sourceMappingURL=makeRagGenerateUserPrompt.d.ts.map