"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.convertMessageFromLlmToDb = exports.streamGenerateResponse = exports.awaitGenerateResponse = exports.generateResponse = void 0;
const services_1 = require("../services");
const utils_1 = require("../utils");
const assert_1 = require("assert");
/**
  Generate a response with/without streaming. Supports tool calling
  and standard response generation.
 */
async function generateResponse({ shouldStream, llm, llmConversation, dataStreamer, references, reqId, llmNotWorkingMessage, noRelevantContentMessage, conversation, request, }) {
    if (shouldStream) {
        return await streamGenerateResponse({
            dataStreamer,
            references,
            reqId,
            llm,
            llmConversation,
            noRelevantContentMessage,
            llmNotWorkingMessage,
            request,
        });
    }
    else {
        return await awaitGenerateResponse({
            references,
            reqId,
            llm,
            llmConversation,
            llmNotWorkingMessage,
            conversation,
            noRelevantContentMessage,
            request,
        });
    }
}
exports.generateResponse = generateResponse;
async function awaitGenerateResponse({ reqId, llmConversation, llm, conversation, references, llmNotWorkingMessage, noRelevantContentMessage, request, }) {
    const newMessages = [];
    const outputReferences = [];
    if (references) {
        outputReferences.push(...references);
    }
    try {
        (0, utils_1.logRequest)({
            reqId,
            message: `All messages for LLM: ${JSON.stringify(llmConversation)}`,
        });
        const answer = await llm.answerQuestionAwaited({
            messages: llmConversation,
        });
        newMessages.push(convertMessageFromLlmToDb(answer));
        // LLM responds with tool call
        if (answer?.functionCall) {
            (0, assert_1.strict)(llm.callTool, "You must implement the callTool() method on your ChatLlm to access this code.");
            const toolAnswer = await llm.callTool({
                messages: [...llmConversation, ...newMessages],
                conversation,
                request,
            });
            (0, utils_1.logRequest)({
                reqId,
                message: `LLM tool call: ${JSON.stringify(toolAnswer)}`,
            });
            const { toolCallMessage, references: toolReferences, rejectUserQuery, } = toolAnswer;
            newMessages.push(convertMessageFromLlmToDb(toolCallMessage));
            // Update references from tool call
            if (toolReferences) {
                outputReferences.push(...toolReferences);
            }
            // Return static response if query rejected
            if (rejectUserQuery) {
                newMessages.push({
                    role: "assistant",
                    content: noRelevantContentMessage,
                });
            } // Otherwise respond with LLM again
            else {
                const answer = await llm.answerQuestionAwaited({
                    messages: [...llmConversation, ...newMessages],
                    // Only allow 1 tool call per user message.
                    toolCallOptions: "none",
                });
                newMessages.push(convertMessageFromLlmToDb(answer));
            }
        }
    }
    catch (err) {
        const errorMessage = err instanceof Error ? err.message : JSON.stringify(err);
        (0, utils_1.logRequest)({
            reqId,
            message: `LLM error: ${errorMessage}`,
            type: "error",
        });
        (0, utils_1.logRequest)({
            reqId,
            message: "Only sending vector search results to user",
        });
        const llmNotWorkingResponse = {
            role: "assistant",
            content: llmNotWorkingMessage,
            references,
        };
        newMessages.push(llmNotWorkingResponse);
    }
    (0, assert_1.strict)(newMessages.length > 0);
    // Add references to the last assistant message
    newMessages[newMessages.length - 1].references =
        outputReferences;
    return { messages: newMessages };
}
exports.awaitGenerateResponse = awaitGenerateResponse;
async function streamGenerateResponse({ dataStreamer, llm, llmConversation, conversation, reqId, references, noRelevantContentMessage, llmNotWorkingMessage, request, }) {
    const newMessages = [];
    const outputReferences = [];
    if (references) {
        outputReferences.push(...references);
    }
    try {
        const answerStream = await llm.answerQuestionStream({
            messages: llmConversation,
        });
        const initialAssistantMessage = {
            role: "assistant",
            content: "",
        };
        const functionCallContent = {
            name: "",
            arguments: "",
        };
        for await (const event of answerStream) {
            if (event.choices.length === 0) {
                continue;
            }
            // The event could contain many choices, but we only want the first one
            const choice = event.choices[0];
            // Assistant response to user
            if (choice.delta?.content) {
                const content = (0, services_1.escapeNewlines)(choice.delta.content ?? "");
                dataStreamer.streamData({
                    type: "delta",
                    data: content,
                });
                initialAssistantMessage.content += content;
            }
            // Tool call
            else if (choice.delta?.functionCall) {
                if (choice.delta?.functionCall.name) {
                    functionCallContent.name += (0, services_1.escapeNewlines)(choice.delta?.functionCall.name ?? "");
                }
                if (choice.delta?.functionCall.arguments) {
                    functionCallContent.arguments += (0, services_1.escapeNewlines)(choice.delta?.functionCall.arguments ?? "");
                }
            }
            else if (choice.message) {
                (0, utils_1.logRequest)({
                    reqId,
                    message: `Unexpected message in stream: no delta. Message: ${JSON.stringify(choice.message)}`,
                    type: "warn",
                });
            }
        }
        const shouldCallTool = functionCallContent.name !== "";
        if (shouldCallTool) {
            initialAssistantMessage.functionCall = functionCallContent;
        }
        newMessages.push(initialAssistantMessage);
        (0, utils_1.logRequest)({
            reqId,
            message: `LLM response: ${JSON.stringify(initialAssistantMessage)}`,
        });
        // Tool call
        if (shouldCallTool) {
            (0, assert_1.strict)(llm.callTool, "You must implement the callTool() method on your ChatLlm to access this code.");
            const { toolCallMessage, references: toolReferences, rejectUserQuery, } = await llm.callTool({
                messages: [...llmConversation, ...newMessages],
                conversation,
                dataStreamer,
                request,
            });
            newMessages.push(convertMessageFromLlmToDb(toolCallMessage));
            if (rejectUserQuery) {
                newMessages.push({
                    role: "assistant",
                    content: noRelevantContentMessage,
                });
                dataStreamer.streamData({
                    type: "delta",
                    data: noRelevantContentMessage,
                });
            }
            else {
                if (toolReferences) {
                    outputReferences.push(...toolReferences);
                }
                const answerStream = await llm.answerQuestionStream({
                    messages: [...llmConversation, ...newMessages],
                });
                const answerContent = await dataStreamer.stream({
                    stream: answerStream,
                });
                const answerMessage = {
                    role: "assistant",
                    content: answerContent,
                };
                newMessages.push(answerMessage);
            }
        }
    }
    catch (err) {
        const errorMessage = err instanceof Error ? err.message : JSON.stringify(err);
        (0, utils_1.logRequest)({
            reqId,
            message: `LLM error: ${errorMessage}`,
            type: "error",
        });
        (0, utils_1.logRequest)({
            reqId,
            message: "Only sending vector search results to user",
        });
        const llmNotWorkingResponse = {
            role: "assistant",
            content: llmNotWorkingMessage,
        };
        dataStreamer.streamData({
            type: "delta",
            data: llmNotWorkingMessage,
        });
        newMessages.push(llmNotWorkingResponse);
    }
    // Stream back references
    dataStreamer.streamData({
        type: "references",
        data: outputReferences,
    });
    (0, assert_1.strict)(newMessages.length > 0);
    // Add references to the last assistant message
    newMessages[newMessages.length - 1].references =
        outputReferences;
    return { messages: newMessages };
}
exports.streamGenerateResponse = streamGenerateResponse;
function convertMessageFromLlmToDb(message) {
    return {
        ...message,
        content: message?.content ?? "",
    };
}
exports.convertMessageFromLlmToDb = convertMessageFromLlmToDb;
//# sourceMappingURL=generateResponse.js.map