"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.makeTypeChatJsonTranslateFunc = void 0;
const typechat_1 = require("typechat");
const exponential_backoff_1 = require("exponential-backoff");
function makeTypeChatJsonTranslateFunc({ azureOpenAiServiceConfig, schema, schemaName, numRetries = 1, retryDelayMs = 1000, }) {
    const { apiKey, baseUrl, deployment, version } = azureOpenAiServiceConfig;
    const model = (0, typechat_1.createAzureOpenAILanguageModel)(apiKey, `${baseUrl}openai/deployments/${deployment}/chat/completions?api-version=${version}`);
    // LLM function
    const translator = (0, typechat_1.createJsonTranslator)(model, schema, schemaName);
    return async (prompt) => {
        const response = await (0, exponential_backoff_1.backOff)(() => translator.translate(prompt), {
            numOfAttempts: numRetries,
            startingDelay: retryDelayMs,
        });
        if (!response.success) {
            throw response; // Response is `Error`
        }
        return response.data; // Success
    };
}
exports.makeTypeChatJsonTranslateFunc = makeTypeChatJsonTranslateFunc;
//# sourceMappingURL=TypeChatJsonTranslateFunc.js.map